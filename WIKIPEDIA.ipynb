{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPUaRUw8TmFvBVsiY8ZjJH"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cLMLHPCSMQL",
        "outputId": "32c60b71-d25e-4c2f-85ed-5806559f608f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia-api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66mZakbsVId4",
        "outputId": "d1204b2f-6aa1-4974-fea5-5249526fe211"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wikipedia-api in /usr/local/lib/python3.9/dist-packages (0.5.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from wikipedia-api) (2.25.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->wikipedia-api) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->wikipedia-api) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->wikipedia-api) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->wikipedia-api) (1.26.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wikipediaapi\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "r0sMZuBPKaI-"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import wikipediaapi\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"/content/gdrive/MyDrive/Colab Notebooks/articleDesc.csv\", sep='[@$@]'  , engine='python', index_col = False)\n",
        "print(data.columns)\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJBqUzpb8YwQ",
        "outputId": "31627feb-4221-4c47-b259-48c817e3a7ae"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py:586: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
            "  return _read(filepath_or_buffer, kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Article Name', 'Unnamed: 1', 'Unnamed: 2', 'Vital Article',\n",
            "       'Unnamed: 4', 'Unnamed: 5', 'Level', 'Unnamed: 7', 'Unnamed: 8',\n",
            "       'Class', 'Unnamed: 10', 'Unnamed: 11', 'Importance', 'Unnamed: 13',\n",
            "       'Unnamed: 14', 'Topic', 'Unnamed: 16', 'Unnamed: 17', 'Wikiproject'],\n",
            "      dtype='object')\n",
            "                                    Article Name Unnamed: 1 Unnamed: 2  \\\n",
            "0  Population_history_of_ancient_Egypt_Archive_8        NaN        NaN   \n",
            "1                                Sin_Mirar_Atr√°s        NaN        NaN   \n",
            "2                                          RMWFC        NaN        NaN   \n",
            "3                               Wilbur_E._Colyer        NaN        NaN   \n",
            "4                                    Salmson_B.9        NaN        NaN   \n",
            "\n",
            "  Vital Article Unnamed: 4  Unnamed: 5 Level  Unnamed: 7  Unnamed: 8  Class  \\\n",
            "0           NaN        NaN         NaN   NaN         NaN         NaN    NaN   \n",
            "1           NaN        NaN         NaN   NaN         NaN         NaN    NaN   \n",
            "2           NaN        NaN         NaN   NaN         NaN         NaN    NaN   \n",
            "3           NaN        NaN         NaN   NaN         NaN         NaN   Stub   \n",
            "4           NaN        NaN         NaN   NaN         NaN         NaN  Start   \n",
            "\n",
            "  Unnamed: 10 Unnamed: 11 Importance Unnamed: 13  Unnamed: 14  \\\n",
            "0         NaN         NaN        NaN         NaN          NaN   \n",
            "1         NaN         NaN        NaN         NaN          NaN   \n",
            "2         NaN         NaN        NaN         NaN          NaN   \n",
            "3         NaN         NaN        NaN         NaN          NaN   \n",
            "4         NaN         NaN        NaN         NaN          NaN   \n",
            "\n",
            "                                               Topic Unnamed: 16 Unnamed: 17  \\\n",
            "0                                                NaN        None        None   \n",
            "1                                                NaN        None        None   \n",
            "2                                                NaN        None        None   \n",
            "3  ['biography', 'military history', 'united stat...        None        None   \n",
            "4                                                NaN        None        None   \n",
            "\n",
            "   Wikiproject  \n",
            "0          NaN  \n",
            "1          NaN  \n",
            "2          NaN  \n",
            "3          NaN  \n",
            "4          NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract a sample of articles\n",
        "np.random.seed(42)\n",
        "sample_size = 500\n",
        "df_sample = data.sample(sample_size)\n",
        "\n",
        "# Initialize the Wikipedia API\n",
        "wiki = wikipediaapi.Wikipedia('en')"
      ],
      "metadata": {
        "id": "t_CvbElu9Ozk"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(article_name):\n",
        "    page = wiki.page(article_name)\n",
        "    if not page.exists():\n",
        "        return None, None\n",
        "    text = page.text\n",
        "    words = word_tokenize(text)\n",
        "    words = [word.lower() for word in words if word.isalpha() and word.lower() not in stopwords.words('english')]\n",
        "    stemmer = PorterStemmer()\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "    text = ' '.join(words)\n",
        "    return text, page.summary"
      ],
      "metadata": {
        "id": "MNUbHz5RJwxW"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "X = []\n",
        "y = []\n",
        "for article_name, article_class, *_ in df_sample.values:\n",
        "    text, summary = extract_features(article_name)\n",
        "    if text is not None and summary is not None:\n",
        "        X.append(text + ' ' + summary)\n",
        "        y.append(article_class)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwf5pjukJynh",
        "outputId": "b4ed2791-f6f1-4a17-aa8d-3e4f6a3a380e"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "rZcIquksLc-y"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n"
      ],
      "metadata": {
        "id": "jvSUP1mALsau"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import wikipediaapi\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"/content/gdrive/MyDrive/Colab Notebooks/articleDesc.csv\", sep='[@$@]'  , engine='python', index_col = False, header=None, names=['Article', 'Class'])\n",
        "#df = pd.read_csv('dataset.csv', delimiter='@$@', header=None, names=['Article', 'Class'])\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# Sample articles\n",
        "df_sample = df.sample(n=100, random_state=42)\n",
        "\n",
        "# Extract article content using Wikipedia API\n",
        "wiki = wikipediaapi.Wikipedia('en')\n",
        "articles = []\n",
        "\n",
        "for article_name in df_sample['Article']:\n",
        "    page = wiki.page(article_name)\n",
        "    if page.exists():\n",
        "        articles.append(page.text)\n",
        "    else:\n",
        "        articles.append('')\n",
        "\n",
        "# Create TF-IDF matrix\n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "X = tfidf.fit_transform(articles)\n",
        "\n",
        "# Create labels\n",
        "y = df_sample['Class'].apply(lambda x: 1 if x == 'FA' else 0)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train classifier\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Test classifier\n",
        "y_pred = clf.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfCmv1e8Mjry",
        "outputId": "194b60e8-cbc6-406b-8350-88e04621bb2f"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py:586: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
            "  return _read(filepath_or_buffer, kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    }
  ]
}